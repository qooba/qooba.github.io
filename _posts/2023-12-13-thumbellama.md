---
id: 744
title: 'Tiny LLama: Compact LLM with WebAssembly'
date: '2023-12-13T08:00:00'
author: qooba
layout: post
guid: 'https://blog.qooba.net/?p=743'
permalink: /2023/12/13/tinyllama-compact-llm-with-webassembly/
categories:
    - tinyllama
    - LLM
    - AI
    - MachineLearning
    - LanguageModels
    - ArtificialIntelligence
    - WebAssembly
tags:
    - ArtificialIntelligence
    - LLM
    - AI
    - MachineLearning
    - LanguageModels
    - TinyLLama
---

<img src="{{ site.relative_url }}assets/images/2023/12/llama.png" alt="all" width="900" />

Tiny LLama is an ambitious initiative aimed at pretraining a language model on 
a dataset of 3 trillion tokens. What sets this project apart is not just 
the size of the data but the efficiency and speed of its processing. 
Utilizing 16 A100-40G GPUs, the training of Tiny LLama started in 
September and is planned to span just 90 days.

Before you will continue reading please watch short introduction: 

<div class="video-container">
    <iframe src="https://www.youtube.com/embed/RDhXRGfeCwk" frameborder="0" allowfullscreen></iframe>
</div>


